{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Collecting a list of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'season_num': 1, 'season_url': 'https://gossipgirl.fandom.com/wiki/Season_1'}, {'season_num': 2, 'season_url': 'https://gossipgirl.fandom.com/wiki/Season_2'}, {'season_num': 3, 'season_url': 'https://gossipgirl.fandom.com/wiki/Season_3'}, {'season_num': 4, 'season_url': 'https://gossipgirl.fandom.com/wiki/Season_4'}, {'season_num': 5, 'season_url': 'https://gossipgirl.fandom.com/wiki/Season_5'}, {'season_num': 6, 'season_url': 'https://gossipgirl.fandom.com/wiki/Season_6'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from scrapy import Selector\n",
    "import json\n",
    "from parsel import Selector\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "base_url = str('https://gossipgirl.fandom.com/wiki/Gossip_Girl_Wiki')\n",
    "response = requests.get(base_url)\n",
    "sel = Selector(text=response.text)\n",
    "tv_show = sel.xpath(\"//title/text()\").get()\n",
    "\n",
    "\n",
    "\n",
    "def get_season_links(base_url):\n",
    "    seasons_list = []\n",
    "\n",
    "    # URL of the page to scrape\n",
    "\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(base_url)\n",
    "\n",
    "    # Initialize a Selector instance with the response body\n",
    "    sel = Selector(text=response.text)\n",
    "\n",
    "    # Use XPath to select the ul element with the specified class\n",
    "    ul_element = sel.xpath('body/div[5]/div[4]/div[2]/header/nav/ul/li[3]/div[2]/ul/li[6]/div/ul')\n",
    "\n",
    "    # Use XPath to select all li elements within the ul element\n",
    "    li_elements = ul_element.xpath('.//li')\n",
    "\n",
    "    # Iterate over each li element and extract its URL and season number\n",
    "    seasons_list = [{'season_num': int(li.xpath('.//span/text()').get().strip().split()[-1]),\n",
    "                     'season_url': li.xpath('.//a/@href').get()} \n",
    "                    for li in li_elements]\n",
    "\n",
    "    return seasons_list\n",
    "\n",
    "seasons_data = get_season_links(base_url)\n",
    "print(seasons_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from scrapy import Selector\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_links_from_season_page(season_url, tv_show, season_num):\n",
    "    links_data = []\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the season URL\n",
    "        response = requests.get(season_url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        # Initialize a Selector instance with the response body\n",
    "        sel = Selector(text=response.text)\n",
    "\n",
    "        # Find the starting point for scraping text\n",
    "        start_tag = sel.xpath('//b').get()\n",
    "        start_index = response.text.find(start_tag) + len(start_tag)\n",
    "\n",
    "        # Create a new Selector starting from the <b> tag\n",
    "        sel = Selector(text=response.text[start_index:])\n",
    "\n",
    "        # Select all paragraph elements containing hyperlinks\n",
    "        paragraphs = sel.xpath('//p')\n",
    "\n",
    "        # Iterate over each paragraph element and extract link information\n",
    "        for paragraph_id, paragraph in enumerate(paragraphs, start=1):\n",
    "            # Select all hyperlinks within the paragraph\n",
    "            links = paragraph.xpath('.//a')\n",
    "\n",
    "            # Iterate over each link in the paragraph\n",
    "            for link in links:\n",
    "                # Extract link title using adjusted XPath expression\n",
    "                link_title = link.xpath('./text() | ./descendant-or-self::*/text()').get()\n",
    "                link_url = link.xpath('./@href').get()\n",
    "                link_url = base_url + link_url\n",
    "\n",
    "                # Append link information to the list\n",
    "                links_data.append({\n",
    "                    'tv_show': tv_show,\n",
    "                    'season_num': season_num,\n",
    "                    'paragraph_id': paragraph_id,\n",
    "                    'link_title': link_title.strip() if link_title else None,\n",
    "                    'link_url': link_url\n",
    "                })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping links from {season_url}: {e}\")\n",
    "\n",
    "    return links_data\n",
    "\n",
    "# Initialize an empty list to store links data for all seasons\n",
    "all_links_data = []\n",
    "for season_data in seasons_data:\n",
    "    season_url = season_data['season_url']\n",
    "    season_num = season_data['season_num']\n",
    "    \n",
    "    # Call the scrape_links_from_season_page function with the current season data\n",
    "    links_data = scrape_links_from_season_page(season_url, tv_show, season_num)\n",
    "    \n",
    "    all_links_data.extend(links_data)\n",
    "\n",
    "# Create a DataFrame from the accumulated links data\n",
    "df_links = pd.DataFrame(all_links_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "file_path = '../task2.csv'\n",
    "df_links.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episode_data(season_url):\n",
    "    all_episodes = {\n",
    "        'episode_num': [],\n",
    "        'episode_url': [],\n",
    "        'episode_title': [],\n",
    "        'air_date': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        tables = pd.read_html(season_url, extract_links='body')\n",
    "        for table in tables:\n",
    "            if 'Title' in table.columns and 'Airdate' in table.columns:\n",
    "                skip_next_row = False\n",
    "                for index, row in table.iterrows():\n",
    "                    if skip_next_row:\n",
    "                        skip_next_row = False\n",
    "                        continue\n",
    "                    if row['Title'] == 'Title':\n",
    "                        skip_next_row = True\n",
    "                        continue\n",
    "                    episode_num = int(row['#'][0])\n",
    "                    episode_title = row['Title'][0] \n",
    "                    air_date_str = row['Airdate'][0] \n",
    "                    if re.match(r\"^\\w+\\s\\d{1,2},\\s\\d{4}$\", air_date_str):\n",
    "                        air_date = datetime.strptime(air_date_str, '%B %d, %Y').date()\n",
    "                    else:\n",
    "                        air_date = air_date_str\n",
    "                    episode_url = row['Title'][1] \n",
    "                    if episode_url is not None:\n",
    "                        episode_url = base_url + episode_url\n",
    "                    all_episodes['episode_num'].append(episode_num)\n",
    "                    all_episodes['episode_url'].append(episode_url)\n",
    "                    all_episodes['episode_title'].append(episode_title)\n",
    "                    all_episodes['air_date'].append(air_date)\n",
    "                all_episodes = {key: value[::2] for key, value in all_episodes.items()}\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting episode data for {season_url}: {e}\")\n",
    "    \n",
    "    return all_episodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting episode data for https://gossipgirl.fandom.com/wiki/Season_1: module 'datetime' has no attribute 'strptime'\n",
      "Error getting episode data for https://gossipgirl.fandom.com/wiki/Season_2: module 'datetime' has no attribute 'strptime'\n",
      "Error getting episode data for https://gossipgirl.fandom.com/wiki/Season_3: module 'datetime' has no attribute 'strptime'\n",
      "Error getting episode data for https://gossipgirl.fandom.com/wiki/Season_4: module 'datetime' has no attribute 'strptime'\n",
      "Error getting episode data for https://gossipgirl.fandom.com/wiki/Season_5: module 'datetime' has no attribute 'strptime'\n",
      "Error getting episode data for https://gossipgirl.fandom.com/wiki/Season_6: module 'datetime' has no attribute 'strptime'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tv_show</th>\n",
       "      <th>season_num</th>\n",
       "      <th>season_url</th>\n",
       "      <th>episode_num</th>\n",
       "      <th>episode_url</th>\n",
       "      <th>episode_title</th>\n",
       "      <th>air_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gossip girl</td>\n",
       "      <td>1</td>\n",
       "      <td>https://gossipgirl.fandom.com/wiki/Season_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gossip girl</td>\n",
       "      <td>2</td>\n",
       "      <td>https://gossipgirl.fandom.com/wiki/Season_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gossip girl</td>\n",
       "      <td>3</td>\n",
       "      <td>https://gossipgirl.fandom.com/wiki/Season_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gossip girl</td>\n",
       "      <td>4</td>\n",
       "      <td>https://gossipgirl.fandom.com/wiki/Season_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gossip girl</td>\n",
       "      <td>5</td>\n",
       "      <td>https://gossipgirl.fandom.com/wiki/Season_5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gossip girl</td>\n",
       "      <td>6</td>\n",
       "      <td>https://gossipgirl.fandom.com/wiki/Season_6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tv_show  season_num                                   season_url  \\\n",
       "0  gossip girl           1  https://gossipgirl.fandom.com/wiki/Season_1   \n",
       "1  gossip girl           2  https://gossipgirl.fandom.com/wiki/Season_2   \n",
       "2  gossip girl           3  https://gossipgirl.fandom.com/wiki/Season_3   \n",
       "3  gossip girl           4  https://gossipgirl.fandom.com/wiki/Season_4   \n",
       "4  gossip girl           5  https://gossipgirl.fandom.com/wiki/Season_5   \n",
       "5  gossip girl           6  https://gossipgirl.fandom.com/wiki/Season_6   \n",
       "\n",
       "  episode_num episode_url episode_title air_date  \n",
       "0         NaN         NaN           NaN      NaN  \n",
       "1         NaN         NaN           NaN      NaN  \n",
       "2         NaN         NaN           NaN      NaN  \n",
       "3         NaN         NaN           NaN      NaN  \n",
       "4         NaN         NaN           NaN      NaN  \n",
       "5         NaN         NaN           NaN      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       tv_show  season_num                                   season_url  \\\n",
      "0  gossip girl           1  https://gossipgirl.fandom.com/wiki/Season_1   \n",
      "1  gossip girl           2  https://gossipgirl.fandom.com/wiki/Season_2   \n",
      "2  gossip girl           3  https://gossipgirl.fandom.com/wiki/Season_3   \n",
      "3  gossip girl           4  https://gossipgirl.fandom.com/wiki/Season_4   \n",
      "4  gossip girl           5  https://gossipgirl.fandom.com/wiki/Season_5   \n",
      "5  gossip girl           6  https://gossipgirl.fandom.com/wiki/Season_6   \n",
      "\n",
      "  episode_num episode_url episode_title air_date  \n",
      "0         NaN         NaN           NaN      NaN  \n",
      "1         NaN         NaN           NaN      NaN  \n",
      "2         NaN         NaN           NaN      NaN  \n",
      "3         NaN         NaN           NaN      NaN  \n",
      "4         NaN         NaN           NaN      NaN  \n",
      "5         NaN         NaN           NaN      NaN  \n"
     ]
    }
   ],
   "source": [
    "# Create an initial data frame with the seasons' links\n",
    "df = pd.DataFrame.from_dict(get_season_links(base_url))\n",
    "\n",
    "# Add the TV show name to the data frame\n",
    "tv_show = 'gossip girl'\n",
    "df['tv_show'] = tv_show\n",
    "\n",
    "\n",
    "# Create a new column with all episode information\n",
    "df['episode_data'] = df['season_url'].apply(get_episode_data)\n",
    "\n",
    "\n",
    "df = (\n",
    "    pd.json_normalize(df['episode_data'])\n",
    "    .join(df.drop(columns='episode_data'))\n",
    "    .explode(['episode_num', 'episode_url', 'episode_title', 'air_date'])\n",
    ")\n",
    "\n",
    "# Re-order the columns\n",
    "ordered_columns = ['tv_show', 'season_num', 'season_url',\n",
    "                   'episode_num', 'episode_url', \n",
    "                   'episode_title', 'air_date']\n",
    "df = df[ordered_columns].copy()\n",
    "display(df)\n",
    "print(df)\n",
    "\n",
    "file_path = '../data.frame.csv'\n",
    "df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  Task 2: Collecting important info from episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    note: the task asks to scrape important info from each synopsis, however my page only contains links in its 'season synopses' - as advised by staff, my task 2 scrapes these links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from scrapy import Selector\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_links_from_season_page(season_url, tv_show, season_num):\n",
    "    links_data = []\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the season URL\n",
    "        response = requests.get(season_url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        # Initialize a Selector instance with the response body\n",
    "        sel = Selector(text=response.text)\n",
    "\n",
    "        # Find the starting point for scraping text\n",
    "        start_tag = sel.xpath('//b').get()\n",
    "        start_index = response.text.find(start_tag) + len(start_tag)\n",
    "\n",
    "        # Create a new Selector starting from the <b> tag\n",
    "        sel = Selector(text=response.text[start_index:])\n",
    "\n",
    "        # Select all paragraph elements containing hyperlinks\n",
    "        paragraphs = sel.xpath('//p')\n",
    "\n",
    "        # Iterate over each paragraph element and extract link information\n",
    "        for paragraph_id, paragraph in enumerate(paragraphs, start=1):\n",
    "            # Select all hyperlinks within the paragraph\n",
    "            links = paragraph.xpath('.//a')\n",
    "\n",
    "            # Iterate over each link in the paragraph\n",
    "            for link in links:\n",
    "                # Extract link title using adjusted XPath expression\n",
    "                link_title = link.xpath('./text() | ./descendant-or-self::*/text()').get()\n",
    "                link_url = link.xpath('./@href').get()\n",
    "                link_url = base_url + link_url\n",
    "\n",
    "                # Append link information to the list\n",
    "                links_data.append({\n",
    "                    'tv_show': tv_show,\n",
    "                    'season_num': season_num,\n",
    "                    'paragraph_id': paragraph_id,\n",
    "                    'link_title': link_title.strip() if link_title else None,\n",
    "                    'link_url': link_url\n",
    "                })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping links from {season_url}: {e}\")\n",
    "\n",
    "    return links_data\n",
    "\n",
    "# Initialize an empty list to store links data for all seasons\n",
    "all_links_data = []\n",
    "for season_data in seasons_data:\n",
    "    season_url = season_data['season_url']\n",
    "    season_num = season_data['season_num']\n",
    "    \n",
    "    links_data = scrape_links_from_season_page(season_url, tv_show, season_num)\n",
    "    \n",
    "    all_links_data.extend(links_data)\n",
    "\n",
    "# Create a DataFrame from the accumulated links data\n",
    "df_links = pd.DataFrame(all_links_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "file_path = '//Users/amimai875/Documents/ds105/DS105W/ds105w-2024-w08-summative-amimai875/task2.csv'\n",
    "df_links.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
